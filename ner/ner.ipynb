{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report as report\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import csv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_EXAMPLES = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(r\"C:\\Users\\gramp\\Downloads\\GoogleNews-vectors-negative300.bin.gz\", binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_path = 'eng.train'\n",
    "csv_training_path = 'eng.train.csv'\n",
    "testpath = '../datasets/NER-test.tsv'\n",
    "\n",
    "\n",
    "def conll_to_csv(conll_path: str, csv_path: str) -> None:\n",
    "    with open(conll_path, 'r', encoding='utf-8') as infile, \\\n",
    "        open(csv_path, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"Word\", \"POS\", \"Chunk\", \"Tag\"])\n",
    "\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('-DOCSTART-'):\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "            if len(parts) != 4:\n",
    "                continue\n",
    "\n",
    "            word, pos, chunk, ner_tag = parts\n",
    "            \n",
    "            # adjust NER tags in CoNLL-2003 format to match NER tags in test Data\n",
    "            if ner_tag == 'B-PER' or ner_tag == 'I-PER':\n",
    "                ner_tag += 'SON'\n",
    "            elif ner_tag == 'B-MISC' or ner_tag == 'I-MISC':\n",
    "                ner_tag = ner_tag[:2] + 'WORK_OF_ART'\n",
    "            writer.writerow([word, pos, chunk, ner_tag])\n",
    "            \n",
    "conll_to_csv(conll_path, csv_training_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set tag distribution:\n",
      "Tag                  | Count      | Proportion (percent)\n",
      "-------------------- | ---------- | ----------\n",
      "B-ORG                |       6321 | 3.10%\n",
      "O                    |     169578 | 83.28%\n",
      "B-WORK_OF_ART        |       3438 | 1.69%\n",
      "B-PERSON             |       6600 | 3.24%\n",
      "I-PERSON             |       4528 | 2.22%\n",
      "B-LOC                |       7140 | 3.51%\n",
      "I-ORG                |       3704 | 1.82%\n",
      "I-WORK_OF_ART        |       1155 | 0.57%\n",
      "I-LOC                |       1157 | 0.57%\n"
     ]
    }
   ],
   "source": [
    "all_tags_training = set()\n",
    "all_tags_testing = set()\n",
    "\n",
    "for row in pandas.read_csv(csv_training_path).itertuples():\n",
    "    all_tags_training.add(row.Tag)\n",
    "\n",
    "for row in pandas.read_table(testpath).itertuples():\n",
    "    all_tags_testing.add(row.BIO_NER_tag)\n",
    "    \n",
    "\n",
    "assert all_tags_training == all_tags_testing\n",
    "\n",
    "\n",
    "counter_training = Counter()\n",
    "for row in pandas.read_csv(csv_training_path).itertuples():\n",
    "    counter_training[row.Tag] += 1\n",
    "\n",
    "print(\"Training set tag distribution:\")\n",
    "print(f'{\"Tag\":20} | {\"Count\":10} | {\"Proportion (percent)\":10}')\n",
    "print('-' * 20, '|', '-' * 10, '|', '-' * 10)\n",
    "for tag in counter_training:\n",
    "    print(f'{tag:20} | {counter_training[tag]:10} | {100*(counter_training[tag] / len(pandas.read_csv(csv_training_path))):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_tokens_from_df(\n",
    "         df: pandas.DataFrame,\n",
    "         test = False\n",
    "      ) -> List[Tuple[str, str]]:\n",
    "    \n",
    "    '''Extracts valid tokens from a DataFrame and returns them as a list of tuples.\n",
    "    \n",
    "    Args:\n",
    "        df: A pandas DataFrame containing token and NER tag columns.\n",
    "        test: A boolean indicating whether the data is test data.\n",
    "        \n",
    "    Returns:\n",
    "        A list of tuples in the form [tokens, NER tag].\n",
    "    '''\n",
    "    valid_tokens = []\n",
    "\n",
    "    tokenword = 'Word'\n",
    "    tagword = 'Tag'\n",
    "    if test:\n",
    "        tokenword = 'token'\n",
    "        tagword = 'BIO_NER_tag'\n",
    "\n",
    "    for row  in df.iterrows():\n",
    "        token = row[1][tokenword]   \n",
    "        ne_label = row[1][tagword]\n",
    "        if token != 'DOCSTART':\n",
    "            valid_tokens.append((token, ne_label))\n",
    "        \n",
    "        if not test:\n",
    "            if len(valid_tokens) >= NUM_TRAINING_EXAMPLES and NUM_TRAINING_EXAMPLES > 0:\n",
    "                break\n",
    "            \n",
    "    return valid_tokens\n",
    "\n",
    "def embeddings_from_valid_tokens(\n",
    "            valid_tokens: List[Tuple[str, str]], \n",
    "            model: gensim.models.KeyedVectors,\n",
    "        ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ''' Extracts embeddings and labels from a list of valid tokens.\n",
    "    \n",
    "    Args:\n",
    "        valid_tokens: A list of tuples in the form [token, NER tag].\n",
    "        model: A gensim word embedding model.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing a numpy array of embeddings and a numpy array of labels.\n",
    "    '''\n",
    "\n",
    "    num_tokens = len(valid_tokens)\n",
    "    \n",
    "    # if the token is not in the model, we use a zero vector\n",
    "    input_vectors = np.zeros((num_tokens, 300))\n",
    "    labels = np.empty(num_tokens, dtype=object)\n",
    "\n",
    "    for i, (token, ne_label) in enumerate(valid_tokens):\n",
    "        if token in model:\n",
    "            input_vectors[i] = model[token]\n",
    "        \n",
    "        labels[i] = ne_label\n",
    "        \n",
    "    return input_vectors, labels\n",
    " \n",
    "def token2features(embs: np.ndarray, i: int) -> np.ndarray:\n",
    "    '''Extracts select embeddings as features from a token and its context.\n",
    "    \n",
    "    Args:\n",
    "        embs: A numpy array of embeddings.\n",
    "        i: An integer representing the index of the token in the embeddings array.\n",
    "        \n",
    "    Returns: A numpy array of embeddings. shape: (3, 300)\n",
    "    '''\n",
    "    \n",
    "    # we basically just layer embedding [i-1], embedding [i], and embedding [i+1] on top of each other\n",
    "    previous_token = embs[i-1] if i > 0 else np.zeros(300)\n",
    "    current_token = embs[i]\n",
    "    next_token = embs[i+1] if i < len(embs) - 1 else np.zeros(300)\n",
    "\n",
    "    features = np.empty((3, 300))\n",
    "\n",
    "    features[0] = previous_token\n",
    "    features[1] = current_token\n",
    "    features[2] = next_token\n",
    "    return features\n",
    " \n",
    "def build_feature_dict(pos_tags: List[str], tokens: List[str], i: int) -> Dict[str, bool]:\n",
    "    \"\"\"Return a dictionary of discrete features for a single token.\n",
    "    \n",
    "    Args:\n",
    "        token: A string representing a single token.\n",
    "        \n",
    "    Returns:\n",
    "        A dictionary of discrete features.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    token = tokens[i]\n",
    "    \n",
    "    prev_tag = pos_tags[i-1] if i > 0 else ''\n",
    "    curr_tag = pos_tags[i]\n",
    "    next_tag = pos_tags[i+1] if i < len(pos_tags) - 1 else ''\n",
    "    \n",
    "    features['is_number'] = token.isnumeric()\n",
    "    features['is_upper'] = token.isupper()\n",
    "    \n",
    "    shape = []\n",
    "    for char in token:\n",
    "        if char.isdigit():\n",
    "            shape.append('9')\n",
    "        elif char.isupper():\n",
    "            shape.append('X')\n",
    "        elif char.islower():\n",
    "            shape.append('x')\n",
    "        else:\n",
    "            shape.append(char)\n",
    "            \n",
    "    features['shape=' + ''.join(shape)] = True\n",
    "    \n",
    "    features['prev_tag=' + prev_tag] = True\n",
    "    features['curr_tag=' + curr_tag] = True\n",
    "    features['next_tag=' + next_tag] = True\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_from_csv(path: str, vectorizer: DictVectorizer, scaler: StandardScaler, test: bool = False, mode: str = 'csv'):\n",
    "    '''Extracts features and labels from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        path: A string representing the path to the CSV file.\n",
    "        vectorizer: A DictVectorizer object.\n",
    "        scaler: A StandardScaler object.\n",
    "        test: A boolean indicating whether the data is test data.\n",
    "        mode: A string indicating the file format. (csv or tsv)\n",
    "        \n",
    "    Returns:\n",
    "        A tuple containing a sparse matrix of features and a numpy array of labels.\n",
    "    '''\n",
    "    \n",
    "    # Comments for clarity of each section.\n",
    "    # Each of these should ideally be functions, but I'm lazy\n",
    "    \n",
    "    # raw is a pandas dataframe\n",
    "    if mode == 'csv':\n",
    "        raw = pandas.read_csv(path, on_bad_lines='warn', encoding = \"ISO-8859-1\")\n",
    "    else:\n",
    "        raw = pandas.read_table(path, on_bad_lines='warn', encoding = \"ISO-8859-1\")\n",
    "        \n",
    "    \n",
    "    # get valid tokens (i.e. tokens that are not DOCSTART, up to NUM_TRAINING_EXAMPLES)\n",
    "    valid_tokens = extract_valid_tokens_from_df(raw, test)\n",
    "    \n",
    "    # get discrete features\n",
    "    tokens = [str(token[0]) for token in valid_tokens if token[0]]\n",
    "    pos_tags = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            pos_tags.append(nltk.pos_tag([token])[0][1])\n",
    "        except:\n",
    "            pos_tags.append('')\n",
    "    \n",
    "\n",
    "    train_discrete_features = [build_feature_dict(pos_tags, tokens, i) for i, _ in enumerate(pos_tags)]\n",
    "    if test:\n",
    "        X_discrete = vectorizer.transform(train_discrete_features)\n",
    "    else:\n",
    "        X_discrete = vectorizer.fit_transform(train_discrete_features)\n",
    "    \n",
    "    # get embeddings and embedding features\n",
    "    vectors, labels = embeddings_from_valid_tokens(valid_tokens, word_embedding_model)\n",
    "    embedding_features = np.zeros((len(vectors), 900))\n",
    "    for i in range(len(vectors)):\n",
    "        ctx = token2features(vectors, i)\n",
    "        embedding_features[i] = ctx.reshape(-1)\n",
    "    \n",
    "    # scale the embedding features from (-1, 1) to (0, 1)\n",
    "    if test:\n",
    "        embedding_features_scaled = scaler.transform(embedding_features)\n",
    "    else:\n",
    "        embedding_features_scaled = scaler.fit_transform(embedding_features)\n",
    "        \n",
    "    # convert to sparse matrix\n",
    "    embedding_features_scaled = csr_matrix(embedding_features_scaled)\n",
    "    full_features = hstack([embedding_features_scaled, X_discrete])\n",
    "    \n",
    "    return full_features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer()\n",
    "scaler = StandardScaler()\n",
    "train_features, train_labels = extract_from_csv(csv_training_path, vectorizer, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203621, 1796)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gramp\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight='balanced')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_classifier = svm.LinearSVC(class_weight='balanced', C=0.1, max_iter=1000)\n",
    "svm_classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features, test_labels = extract_from_csv(testpath, vectorizer, scaler, test=True, mode='tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237, 1796)\n",
      "(237,)\n"
     ]
    }
   ],
   "source": [
    "print(test_features.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm_classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "        B-LOC       0.83      0.71      0.77         7\n",
      "        B-ORG       0.33      1.00      0.50         3\n",
      "     B-PERSON       0.67      0.73      0.70        11\n",
      "B-WORK_OF_ART       0.25      0.11      0.15         9\n",
      "        I-LOC       0.25      1.00      0.40         1\n",
      "        I-ORG       0.40      1.00      0.57         2\n",
      "     I-PERSON       0.88      0.88      0.88         8\n",
      "I-WORK_OF_ART       0.00      0.00      0.00        10\n",
      "            O       0.96      0.97      0.97       186\n",
      "\n",
      "     accuracy                           0.88       237\n",
      "    macro avg       0.51      0.71      0.55       237\n",
      " weighted avg       0.85      0.88      0.86       237\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gramp\\anaconda3\\envs\\textmining\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_report = report(test_labels, predictions)\n",
    "print(eval_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_report_as_table(y_true, y_pred, output_path=\"classification_report.png\"):\n",
    "    # Convert classification report to DataFrame\n",
    "    report_dict = report(y_true, y_pred, output_dict=True)\n",
    "    df = pandas.DataFrame(report_dict).transpose().round(2)\n",
    "    df = df[['precision', 'recall', 'f1-score', 'support']]\n",
    "\n",
    "    # Set up figure\n",
    "    fig, ax = plt.subplots(figsize=(10, len(df) * 0.5 + 1.5))\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Create table\n",
    "    table = ax.table(cellText=df.values,\n",
    "                     colLabels=df.columns,\n",
    "                     rowLabels=df.index,\n",
    "                     loc='center',\n",
    "                     cellLoc='center',\n",
    "                     rowLoc='center')\n",
    "\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.1, 1.4)\n",
    "\n",
    "    # Add title closer to the table using fig.suptitle instead of ax.set_title\n",
    "    fig.suptitle(\"Classification Report\", fontsize=14, fontweight='bold', y=0.95)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "save_report_as_table(test_labels, predictions, output_path=\"classification_report.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.83      0.71      0.77         7\n",
      "       B-ORG       0.60      1.00      0.75         3\n",
      "    B-PERSON       0.89      0.80      0.84        10\n",
      "       I-LOC       0.33      1.00      0.50         1\n",
      "       I-ORG       0.50      1.00      0.67         2\n",
      "    I-PERSON       1.00      0.88      0.93         8\n",
      "           O       0.99      0.98      0.99       185\n",
      "\n",
      "    accuracy                           0.96       216\n",
      "   macro avg       0.74      0.91      0.78       216\n",
      "weighted avg       0.97      0.96      0.96       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wwa_test_labels = []\n",
    "wwa_predictions = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i][2:] != 'WORK_OF_ART' and predictions[i][2:] != 'WORK_OF_ART':\n",
    "        wwa_test_labels.append(test_labels[i])\n",
    "        wwa_predictions.append(predictions[i])\n",
    "\n",
    "\n",
    "wwa_eval_report = report(wwa_test_labels, wwa_predictions)\n",
    "print(wwa_eval_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NE       0.90      0.84      0.87        51\n",
      "           O       0.96      0.97      0.97       186\n",
      "\n",
      "    accuracy                           0.95       237\n",
      "   macro avg       0.93      0.91      0.92       237\n",
      "weighted avg       0.94      0.95      0.94       237\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NER_test_labels = []\n",
    "NER_predictions = []\n",
    "for i in range(len(test_labels)):\n",
    "    NER_test_labels.append(test_labels[i] if test_labels[i] == 'O' else \"NE\")\n",
    "    NER_predictions.append(predictions[i] if predictions[i] == 'O' else \"NE\")\n",
    "\n",
    "\n",
    "NER_eval_report = report(NER_test_labels, NER_predictions)\n",
    "print(NER_eval_report)\n",
    "\n",
    "save_report_as_table(NER_test_labels, NER_predictions, output_path=\"NER_only.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True: B-LOC, Pred: B-ORG, Context: ['moved', 'to', 'Barcelona', 'last', 'summer']\n",
      "True: B-LOC, Pred: I-ORG, Context: ['at', 'Wembley', 'Stadium', 'was', 'absolutely']\n",
      "True: O, Pred: B-ORG, Context: ['is', 'the', 'GOAT', '.', 'I']\n",
      "True: B-WORK_OF_ART, Pred: O, Context: ['finished', 'reading', 'The', 'Catcher', 'in']\n",
      "True: I-WORK_OF_ART, Pred: I-LOC, Context: ['reading', 'The', 'Catcher', 'in', 'the']\n",
      "True: I-WORK_OF_ART, Pred: O, Context: ['The', 'Catcher', 'in', 'the', 'Rye']\n",
      "True: I-WORK_OF_ART, Pred: O, Context: ['Catcher', 'in', 'the', 'Rye', 'and']\n",
      "True: I-WORK_OF_ART, Pred: B-ORG, Context: ['in', 'the', 'Rye', 'and', \"couldn't\"]\n",
      "True: B-PERSON, Pred: B-WORK_OF_ART, Context: ['spotted', 'a', 'Banksy', 'mural', 'whilst']\n",
      "True: B-WORK_OF_ART, Pred: B-PERSON, Context: ['!', 'The', 'Harry', 'Potter', 'series']\n",
      "True: I-WORK_OF_ART, Pred: I-PERSON, Context: ['The', 'Harry', 'Potter', 'series', 'will']\n",
      "True: O, Pred: I-LOC, Context: ['be', 'my', 'go-to', 'comfort', 'read']\n",
      "True: O, Pred: B-WORK_OF_ART, Context: ['much', 'about', 'German', 'history', '.']\n",
      "True: B-WORK_OF_ART, Pred: B-ORG, Context: ['finished', 'watching', 'Stranger', 'Things', ',']\n",
      "True: I-WORK_OF_ART, Pred: I-ORG, Context: ['watching', 'Stranger', 'Things', ',', 'and']\n",
      "True: B-PERSON, Pred: O, Context: [',', 'and', 'Eleven', 'is', 'hands']\n",
      "True: B-WORK_OF_ART, Pred: O, Context: ['started', 'reading', '1984', ',', 'and']\n",
      "True: O, Pred: I-LOC, Context: ['George', 'Orwell', \"'s\", 'vision', 'of']\n",
      "True: B-WORK_OF_ART, Pred: B-PERSON, Context: ['chilling', '.', 'Barbie', 'and', 'Oppenheimer']\n",
      "True: O, Pred: I-ORG, Context: ['.', 'Barbie', 'and', 'Oppenheimer', 'releasing']\n",
      "True: B-WORK_OF_ART, Pred: B-PERSON, Context: ['Barbie', 'and', 'Oppenheimer', 'releasing', 'on']\n",
      "True: B-WORK_OF_ART, Pred: O, Context: ['edition', 'of', 'To', 'Kill', 'a']\n",
      "True: I-WORK_OF_ART, Pred: B-WORK_OF_ART, Context: ['of', 'To', 'Kill', 'a', 'Mockingbird']\n",
      "True: I-WORK_OF_ART, Pred: O, Context: ['To', 'Kill', 'a', 'Mockingbird', 'at']\n",
      "True: I-WORK_OF_ART, Pred: B-ORG, Context: ['Kill', 'a', 'Mockingbird', 'at', 'a']\n",
      "True: B-WORK_OF_ART, Pred: O, Context: ['just', 'binged', 'The', 'Crown', ',']\n",
      "True: I-WORK_OF_ART, Pred: B-ORG, Context: ['binged', 'The', 'Crown', ',', 'and']\n",
      "True: B-PERSON, Pred: B-LOC, Context: ['Foy', 'as', 'Queen', 'Elizabeth', 'II']\n",
      "True: I-PERSON, Pred: B-PERSON, Context: ['as', 'Queen', 'Elizabeth', 'II', 'was']\n"
     ]
    }
   ],
   "source": [
    "tokens = extract_valid_tokens_from_df(pandas.read_table(testpath, on_bad_lines='warn', encoding = \"ISO-8859-1\"), test=True)\n",
    "test_tokens = [token[0] for token in tokens if token[0]]\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] != predictions[i]:\n",
    "        print(f\"True: {test_labels[i]}, Pred: {predictions[i]}, Context: {test_tokens[i-2 :i+3]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
